<img src='https://cdn.vox-cdn.com/thumbor/WMzDINEzBqNv9fk38mYW6FxHYaw=/0x0:2040x1360/1200x675/filters:focal(857x517:1183x843)/cdn.vox-cdn.com/uploads/chorus_image/image/69705444/acstro_190902_apple_event_0004.0.0.jpg' width='700px' /><br/>
Last week, Apple, without very much warning at all, announced a new set of tools built into the iPhone designed to protect children from abuse. Siri will now offer resources to people who ask for child abuse material or who ask how to report it. iMessage will now flag nudes sent or received by kids under 13 and alert their parents. Images backed up to iCloud Photos will now be matched against a database of known child sexual abuse material (CSAM) and reported to the National Center for Missing and Exploited Children (NCMEC) if more than a certain number of images match. And that matching process doesn't just happen in the cloud â€” part of it happens locally on your phone. That's a big change from how things normally work.
<a href='https://www.theverge.com/22617554/apple-csam-child-safety-features-jen-king-riana-pfefferkorn-interview-decoder'> Source <a/>