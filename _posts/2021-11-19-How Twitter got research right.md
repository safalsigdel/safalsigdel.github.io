<img src='https://cdn.vox-cdn.com/thumbor/AT6A74N-OrCOIMB2TNRasaF77w0=/0x0:2040x1360/1200x800/filters:focal(857x517:1183x843)/cdn.vox-cdn.com/uploads/chorus_image/image/70164347/acastro_200715_1777_twitter_0005.0.0.jpg' width='700px' /><br/>
It has not been a happy time for researchers at big tech companies. Hired to help executives understand platforms' shortcomings, research teams inevitably reveal inconvenient truths. Companies hire teams to build “responsible AI” but bristle when their employees discover algorithmic bias. They boast about the quality of their internal research but disavow it when it makes its way to the press. At Google, this story played out in the forced departure of ethical AI researcher Timnit Gebru and the subsequent fallout for her team. At Facebook, it led to Frances Haugen and the Facebook Files.
<a href='https://www.theverge.com/2021/11/19/22790174/twitter-research-transparency-published-findings'> Source <a/>