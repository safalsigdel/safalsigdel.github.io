<img src='https://cdn.vox-cdn.com/thumbor/nthoSJfR3CJhjaxZ60uT3prhxkA=/0x0:3000x2000/1200x800/filters:focal(1260x760:1740x1240)/cdn.vox-cdn.com/uploads/chorus_image/image/70696460/acastro_180928_1777_facebook_hack_0001.0.jpg' width='700px' /><br/>
A major responsibility for tech companies is to monitor content on their platforms for child sexual abuse material (CSAM), and if any is found, they are legally required to report it to  the National Center for Missing and Exploited Children (NCMEC). Many companies have content moderators in place that review content flagged for potentially being CSAM, and they determine whether the content should be reported to the NCMEC.
<a href='https://www.theverge.com/2022/3/31/23005576/facebook-content-moderators-child-sexual-abuse-material-csam-policy'> Source <a/>