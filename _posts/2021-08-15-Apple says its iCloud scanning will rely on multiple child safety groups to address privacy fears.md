<img src='https://cdn.vox-cdn.com/thumbor/zO9akiuvbAjBHIpL3uRBbEIvMZc=/0x0:2040x1360/1200x800/filters:focal(857x517:1183x843)/cdn.vox-cdn.com/uploads/chorus_image/image/69723201/acastro_20200818_1777_epicApple_0003.0.0.jpg' width='700px' /><br/>
Apple has filled in more details around its upcoming plans to scan iCloud Photos for child sexual abuse material (CSAM) via users' iPhones and iPads. The company released a new paper delving into the safeguards it hopes will increase user trust in the initiative. That includes a rule to only flag images found in multiple child safety databases with different government affiliations â€” theoretically stopping one country from adding non-CSAM content to the system.
<a href='https://www.theverge.com/2021/8/13/22623859/apple-icloud-photos-csam-scanning-security-multiple-jurisdictions-safeguard'> Source <a/>